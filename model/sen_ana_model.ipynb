{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data Analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"C:/Users/ashva/Projects/sentiment_analysis/Twitter US Airline Sentiment/Tweets.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# need the text and sentiment column.\n",
    "review_df = df[['text','airline_sentiment']]\n",
    "\n",
    "print(review_df.shape)\n",
    "review_df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check the values of the airline_sentiment column.\n",
    "review_df[\"airline_sentiment\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# convert the categorical values to numeric using the \"factorize()\" method\n",
    "sentiment_label = review_df.airline_sentiment.factorize()\n",
    "# sentiment_label[0].shape\n",
    "# sentiment_label[1].shape\n",
    "# sentiment_label[0]\n",
    "# sentiment_label[1]\n",
    "sentiment_label"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# retrieve all the text data from the dataset\n",
    "tweet = review_df.text.values\n",
    "\n",
    "# break down all the words/sentences of a text into small parts called tokens\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words=7436)\n",
    "\n",
    "# create an association between the words and the assigned numbers using fit_on_texts\n",
    "tokenizer.fit_on_texts(tweet)\n",
    "\n",
    "# store associations in the form of a dictionary in the tokenizer.word_index attribute\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# replace the words with their assigned numbers using the text_to_sequence() method\n",
    "encoded_docs = tokenizer.texts_to_sequences(tweet)\n",
    "\n",
    "# pad the sentences to have equal length\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "def features_extractor(i_num):\n",
    "    padded_sequence = pad_sequences(encoded_docs, maxlen=100)\n",
    "    return padded_sequence[i_num]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# we need to extract the featured from all the tweets, so we use tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Now we iterate through every tweet and extract features\n",
    "# using Tokenizer\n",
    "\n",
    "extracted_features=[]\n",
    "for index_num,row in tqdm(review_df.iterrows()):\n",
    "    final_class_labels=row[\"airline_sentiment\"]\n",
    "    data=features_extractor(index_num)\n",
    "    extracted_features.append([data,final_class_labels])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# converting extracted_features to Pandas dataframe\n",
    "extracted_features_df=pd.DataFrame(extracted_features,columns=['feature','class'])\n",
    "extracted_features_df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Split Processed Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Split the dataset into independent and dependent dataset\n",
    "X=np.array(extracted_features_df['feature'].tolist())\n",
    "y=np.array(extracted_features_df['class'].tolist())\n",
    "\n",
    "# Label Encoding -> Label Encoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder=LabelEncoder()\n",
    "y=to_categorical(label_encoder.fit_transform(y))\n",
    "\n",
    "# Train Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2, random_state=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Building Text Classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Activation, SpatialDropout1D\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "num_labels = sentiment_label[1].shape[0]\n",
    "print(num_labels)\n",
    "\n",
    "embedding_vector_length = 32\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(vocab_size, embedding_vector_length, input_length=100))\n",
    "model.add(SpatialDropout1D(0.25))\n",
    "\n",
    "model.add(LSTM(50, dropout=0.25, recurrent_dropout=0.25))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy', 'Precision', 'Recall'])\n",
    "\n",
    "print(model.summary())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Training my model\n",
    "import os\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from datetime import datetime\n",
    "\n",
    "num_epochs = 5\n",
    "num_batch_size = 32\n",
    "\n",
    "checkpoint_path = \"training_1/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "cp_callback = ModelCheckpoint(filepath=checkpoint_path, verbose=1, save_best_only=True)\n",
    "\n",
    "start = datetime.now()\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(X_test, y_test), callbacks=[cp_callback], verbose=1)\n",
    "\n",
    "# creates a single collection of TensorFlow checkpoint files that are updated at the end of each epoch\n",
    "os.listdir(checkpoint_dir)\n",
    "\n",
    "# Save the entire model to a HDF5 file.\n",
    "# The '.h5' extension indicates that the model should be saved to HDF5.\n",
    "model.save('my_sen_ana_model.h5')\n",
    "\n",
    "duration = datetime.now() - start\n",
    "\n",
    "print(\"Training completed in time: \", duration)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "test_accuracy =model.evaluate(X_test,y_test,verbose=1)\n",
    "print(\"Accuracy: {:.2f}%\".format(test_accuracy[1]*100))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#model.predict_classes(X_test)\n",
    "import numpy as np\n",
    "\n",
    "predict_x=model.predict(X_test)\n",
    "classes_x=np.argmax(predict_x,axis=1)\n",
    "print(classes_x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visualizing the metrics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.subplot(211)\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "\n",
    "plt.title('Model Accuracy')\n",
    "\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.legend(['Training', 'Validation'], loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.savefig(\"model/images/Accuracy plot.jpg\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.subplot(212)\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "\n",
    "plt.title('Model Loss')\n",
    "\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.legend(['Training', 'Validation'], loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.savefig(\"model/images/Loss plt.jpg\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import scikitplot as skplt\n",
    "\n",
    "# convert tests labels in single-digits instead of one-hot encoding!\n",
    "y_test_arg=np.argmax(y_test,axis=1)\n",
    "skplt.metrics.plot_confusion_matrix(y_test_arg, classes_x, normalize=False, title = 'Confusion Matrix for CAC w/o norm')\n",
    "\n",
    "plt.savefig(\"model/images/Confusion Matrix for CAC w/o norm.jpg\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "skplt.metrics.plot_confusion_matrix(y_test_arg, classes_x, normalize=True, title = 'Confusion Matrix for CAC with norm')\n",
    "\n",
    "plt.savefig(\"model/images/Confusion Matrix for CAC with norm.jpg\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize = (18,8))\n",
    "sns.heatmap(skplt.metrics.confusion_matrix(y_test_arg, classes_x, normalize='true'), annot = True, cmap = 'plasma')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n",
    "\n",
    "plt.savefig(\"model/images/Confusion Matrix for CAC with norm(sns).jpg\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Execution"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def predict_sentiment(text):\n",
    "    tw = tokenizer.texts_to_sequences([text])\n",
    "    tw = pad_sequences(tw,maxlen=200)\n",
    "    prediction = int(model.predict(tw).round().item())\n",
    "    print(\"Predicted label: \", sentiment_label[1][prediction])\n",
    "\n",
    "choice = 'y'\n",
    "\n",
    "while 'y':\n",
    "    test_sentence1 = input(\"Enter statement: \")\n",
    "    predict_sentiment(test_sentence1)\n",
    "    choice = input(\"Continue (y/n) ?\")\n",
    "    if choice == 'n':\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
