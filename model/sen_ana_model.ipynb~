{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data Analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"C:/Users/ashva/Projects/sentiment_analysis/Twitter US Airline Sentiment/Tweets.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n0  570306133677760513           neutral                        1.0000   \n1  570301130888122368          positive                        0.3486   \n2  570301083672813571           neutral                        0.6837   \n3  570301031407624196          negative                        1.0000   \n4  570300817074462722          negative                        1.0000   \n\n  negativereason  negativereason_confidence         airline  \\\n0            NaN                        NaN  Virgin America   \n1            NaN                     0.0000  Virgin America   \n2            NaN                        NaN  Virgin America   \n3     Bad Flight                     0.7033  Virgin America   \n4     Can't Tell                     1.0000  Virgin America   \n\n  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n0                    NaN     cairdin                 NaN              0   \n1                    NaN    jnardino                 NaN              0   \n2                    NaN  yvonnalynn                 NaN              0   \n3                    NaN    jnardino                 NaN              0   \n4                    NaN    jnardino                 NaN              0   \n\n                                                text tweet_coord  \\\n0                @VirginAmerica What @dhepburn said.         NaN   \n1  @VirginAmerica plus you've added commercials t...         NaN   \n2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n3  @VirginAmerica it's really aggressive to blast...         NaN   \n4  @VirginAmerica and it's a really big bad thing...         NaN   \n\n               tweet_created tweet_location               user_timezone  \n0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet_id</th>\n      <th>airline_sentiment</th>\n      <th>airline_sentiment_confidence</th>\n      <th>negativereason</th>\n      <th>negativereason_confidence</th>\n      <th>airline</th>\n      <th>airline_sentiment_gold</th>\n      <th>name</th>\n      <th>negativereason_gold</th>\n      <th>retweet_count</th>\n      <th>text</th>\n      <th>tweet_coord</th>\n      <th>tweet_created</th>\n      <th>tweet_location</th>\n      <th>user_timezone</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>570306133677760513</td>\n      <td>neutral</td>\n      <td>1.0000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Virgin America</td>\n      <td>NaN</td>\n      <td>cairdin</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>@VirginAmerica What @dhepburn said.</td>\n      <td>NaN</td>\n      <td>2015-02-24 11:35:52 -0800</td>\n      <td>NaN</td>\n      <td>Eastern Time (US &amp; Canada)</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>570301130888122368</td>\n      <td>positive</td>\n      <td>0.3486</td>\n      <td>NaN</td>\n      <td>0.0000</td>\n      <td>Virgin America</td>\n      <td>NaN</td>\n      <td>jnardino</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>@VirginAmerica plus you've added commercials t...</td>\n      <td>NaN</td>\n      <td>2015-02-24 11:15:59 -0800</td>\n      <td>NaN</td>\n      <td>Pacific Time (US &amp; Canada)</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>570301083672813571</td>\n      <td>neutral</td>\n      <td>0.6837</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Virgin America</td>\n      <td>NaN</td>\n      <td>yvonnalynn</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n      <td>NaN</td>\n      <td>2015-02-24 11:15:48 -0800</td>\n      <td>Lets Play</td>\n      <td>Central Time (US &amp; Canada)</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>570301031407624196</td>\n      <td>negative</td>\n      <td>1.0000</td>\n      <td>Bad Flight</td>\n      <td>0.7033</td>\n      <td>Virgin America</td>\n      <td>NaN</td>\n      <td>jnardino</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>@VirginAmerica it's really aggressive to blast...</td>\n      <td>NaN</td>\n      <td>2015-02-24 11:15:36 -0800</td>\n      <td>NaN</td>\n      <td>Pacific Time (US &amp; Canada)</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>570300817074462722</td>\n      <td>negative</td>\n      <td>1.0000</td>\n      <td>Can't Tell</td>\n      <td>1.0000</td>\n      <td>Virgin America</td>\n      <td>NaN</td>\n      <td>jnardino</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>@VirginAmerica and it's a really big bad thing...</td>\n      <td>NaN</td>\n      <td>2015-02-24 11:14:45 -0800</td>\n      <td>NaN</td>\n      <td>Pacific Time (US &amp; Canada)</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['tweet_id', 'airline_sentiment', 'airline_sentiment_confidence',\n       'negativereason', 'negativereason_confidence', 'airline',\n       'airline_sentiment_gold', 'name', 'negativereason_gold',\n       'retweet_count', 'text', 'tweet_coord', 'tweet_created',\n       'tweet_location', 'user_timezone'],\n      dtype='object')"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14200, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                                text airline_sentiment\n0                @VirginAmerica What @dhepburn said.           neutral\n1  @VirginAmerica plus you've added commercials t...          positive\n2  @VirginAmerica I didn't today... Must mean I n...           neutral\n3  @VirginAmerica it's really aggressive to blast...          negative\n4  @VirginAmerica and it's a really big bad thing...          negative",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>airline_sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>@VirginAmerica What @dhepburn said.</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>@VirginAmerica plus you've added commercials t...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>@VirginAmerica it's really aggressive to blast...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>@VirginAmerica and it's a really big bad thing...</td>\n      <td>negative</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# splitting dataframe by row index\n",
    "df_split  = df.iloc[:14200,:]\n",
    "# df_split = df.sample(frac=.5)\n",
    "\n",
    "# need the text and sentiment column.\\\n",
    "review_df = df_split[['text','airline_sentiment']]\n",
    "\n",
    "print(review_df.shape)\n",
    "review_df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "negative    8828\nneutral     3051\npositive    2321\nName: airline_sentiment, dtype: int64"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the values of the airline_sentiment column.\n",
    "review_df[\"airline_sentiment\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "(array([0, 1, 0, ..., 0, 1, 0], dtype=int64),\n Index(['neutral', 'positive', 'negative'], dtype='object'))"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the categorical values to numeric using the \"factorize()\" method\n",
    "sentiment_label = review_df.airline_sentiment.factorize()\n",
    "# sentiment_label[0].shape\n",
    "# sentiment_label[1].shape\n",
    "# sentiment_label[0]\n",
    "# sentiment_label[1]\n",
    "sentiment_label"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# retrieve all the text data from the dataset\n",
    "tweet = review_df.text.values\n",
    "\n",
    "# break down all the words/sentences of a text into small parts called tokens\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "\n",
    "# create an association between the words and the assigned numbers using fit_on_texts\n",
    "tokenizer.fit_on_texts(tweet)\n",
    "\n",
    "# store associations in the form of a dictionary in the tokenizer.word_index attribute\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# replace the words with their assigned numbers using the text_to_sequence() method\n",
    "encoded_docs = tokenizer.texts_to_sequences(tweet)\n",
    "\n",
    "# pad the sentences to have equal length\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "def features_extractor(i_num):\n",
    "    padded_sequence = pad_sequences(encoded_docs, maxlen=200)\n",
    "    return padded_sequence[i_num]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "572it [01:27,  6.58it/s]"
     ]
    }
   ],
   "source": [
    "# we need to extract the featured from all the tweets, so we use tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Now we iterate through every tweet and extract features\n",
    "# using Tokenizer\n",
    "\n",
    "extracted_features=[]\n",
    "for index_num,row in tqdm(review_df.iterrows()):\n",
    "    final_class_labels=row[\"airline_sentiment\"]\n",
    "    data=features_extractor(index_num)\n",
    "    extracted_features.append([data,final_class_labels])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# converting extracted_features to Pandas dataframe\n",
    "extracted_features_df=pd.DataFrame(extracted_features,columns=['feature','class'])\n",
    "extracted_features_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Split Processed Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Split the dataset into independent and dependent dataset\n",
    "X=np.array(extracted_features_df['feature'].tolist())\n",
    "y=np.array(extracted_features_df['class'].tolist())\n",
    "\n",
    "# Label Encoding -> Label Encoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder=LabelEncoder()\n",
    "y=to_categorical(label_encoder.fit_transform(y))\n",
    "\n",
    "# Train Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Building Text Classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Activation, SpatialDropout1D\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "num_labels = sentiment_label[1].shape[0]\n",
    "print(num_labels)\n",
    "\n",
    "embedding_vector_length = 32\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(vocab_size, embedding_vector_length, input_length=200))\n",
    "model.add(SpatialDropout1D(0.25))\n",
    "\n",
    "model.add(LSTM(50, dropout=0.5, recurrent_dropout=0.5))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy', 'Precision', 'Recall'])\n",
    "\n",
    "print(model.summary())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Training my model\n",
    "import os\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from datetime import datetime\n",
    "\n",
    "num_epochs = 10\n",
    "num_batch_size = 32\n",
    "\n",
    "checkpoint_path = \"model/training_1/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "cp_callback = ModelCheckpoint(filepath=checkpoint_path, verbose=1, save_best_only=True)\n",
    "\n",
    "start = datetime.now()\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(X_test, y_test), callbacks=[cp_callback], verbose=1)\n",
    "\n",
    "# creates a single collection of TensorFlow checkpoint files that are updated at the end of each epoch\n",
    "os.listdir(checkpoint_dir)\n",
    "\n",
    "# Save the entire model to a HDF5 file.\n",
    "# The '.h5' extension indicates that the model should be saved to HDF5.\n",
    "model.save('my_sen_ana_model.h5')\n",
    "\n",
    "duration = datetime.now() - start\n",
    "\n",
    "print(\"Training completed in time: \", duration)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#model.predict_classes(X_test)\n",
    "import numpy as np\n",
    "\n",
    "predict_x=model.predict(X_test)\n",
    "classes_x=np.argmax(predict_x,axis=1)\n",
    "print(classes_x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visualizing the metrics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "\n",
    "plt.title('Model Accuracy')\n",
    "\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.legend(['Training', 'Validation'], loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.savefig(\"Accuracy plot.jpg\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "\n",
    "plt.title('Model Loss')\n",
    "\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.legend(['Training', 'Validation'], loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.savefig(\"Loss plt.jpg\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import scikitplot as skplt\n",
    "\n",
    "# convert tests labels in single-digits instead of one-hot encoding!\n",
    "y_test_arg=np.argmax(y_test,axis=1)\n",
    "skplt.metrics.plot_confusion_matrix(y_test_arg, classes_x, normalize=False, title = 'Confusion Matrix for CAC w/o norm')\n",
    "\n",
    "# plt.savefig(\"Confusion Matrix for CAC w/o norm.jpg\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "skplt.metrics.plot_confusion_matrix(y_test_arg, classes_x, normalize=True, title = 'Confusion Matrix for CAC with norm')\n",
    "\n",
    "# plt.savefig(\"Confusion Matrix for CAC with norm.jpg\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize = (18,8))\n",
    "sns.heatmap(skplt.metrics.confusion_matrix(y_test_arg, classes_x, normalize='true'), annot = True, cmap = 'plasma')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n",
    "\n",
    "# plt.savefig(\"Confusion Matrix for CAC with norm(sns).jpg\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Execution"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "new_model = tf.keras.models.load_model('my_sen_ana_model.h5')\n",
    "\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# tokenizer = Tokenizer(num_words=5000)\n",
    "\n",
    "import numpy as np\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "def predict_sentiment(text):\n",
    "    tw = tokenizer.texts_to_sequences([text])\n",
    "    tw = pad_sequences(tw,maxlen=200)\n",
    "    # prediction = int(model.predict(tw).round().item())\n",
    "    x_predict=new_model.predict(tw)\n",
    "    predicted_label=np.argmax(x_predict,axis=1)\n",
    "    # print(\"Predicted label: \", predicted_label)\n",
    "    # prediction_class = LabelEncoder.inverse_transform(predicted_label)\n",
    "    return predicted_label\n",
    "\n",
    "choice = 'y'\n",
    "\n",
    "while 'y':\n",
    "    test_sentence1 = input(\"Enter statement: \")\n",
    "    print(\"Text: \", test_sentence1)\n",
    "    result = predict_sentiment(test_sentence1)\n",
    "    if result == [0]:\n",
    "        print(\"Predicted label: Neutral\")\n",
    "    elif result == [1]:\n",
    "        print(\"Predicted label: Positive\")\n",
    "    else:\n",
    "        print(\"Predicted label: Negative\")\n",
    "    choice = input(\"Continue (y/n) ?\")\n",
    "    if choice == 'n':\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
